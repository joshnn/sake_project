{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, List, Union, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import  DataLoader\n",
    "\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "from transformers.pytorch_utils import apply_chunking_to_forward\n",
    "from transformers.activations import ACT2FN\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics as tm\n",
    "# import bitsandbytes as bnb\n",
    "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
    "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
    "NODE_OP_CODES = 120\n",
    "NODE_FEATS = 140\n",
    "CONFIG_FEATS = 24\n",
    "NODE_CONFIG_FEATS = 18\n",
    "DATA_DIR = \"../input/predict-ai-model-runtime/npz_all/npz\"\n",
    "\n",
    "\n",
    "def generate_tile_df() -> pd.DataFrame:\n",
    "    tile_df = pd.DataFrame({'paths': [elem for elem in (Path(DATA_DIR) / 'tile').rglob(\"*\") if elem.is_file()]}).assign(\n",
    "        split=lambda df: df.paths.apply(lambda x: x.parent.name),\n",
    "        configuration=lambda df: df.paths.apply(lambda x: x.parent.parent.name),\n",
    "        extra=lambda df: df.paths.apply(lambda x: x.parent.parent.parent.name),\n",
    "        model_name=lambda df: df.paths.apply(lambda x: x.stem),\n",
    "        collection=lambda df: df.extra + ':' + df.configuration ,\n",
    "        ID=lambda df: df.collection + ':' + df.model_name ,\n",
    "        paths = lambda df: df.paths.apply(lambda x: str(x))\n",
    "    )\n",
    "    return tile_df\n",
    "tile_df = generate_tile_df()\n",
    "tile_df.head()\n",
    "\"\"\"\n",
    "paths\tsplit\tconfiguration\textra\tmodel_name\tcollection\tID\n",
    "0\t../input/predict-ai-model-runtime/npz_all/npz/...\tvalid\txla\ttile\tresnet_v1_50_official_batch_128_bf16_2bea628b7...\ttile:xla\ttile:xla:resnet_v1_50_official_batch_128_bf16_...\n",
    "1\t../input/predict-ai-model-runtime/npz_all/npz/...\tvalid\txla\ttile\tinception_v3_batch_128_train_40fa8f86f121f00a\ttile:xla\ttile:xla:inception_v3_batch_128_train_40fa8f86...\n",
    "2\t../input/predict-ai-model-runtime/npz_all/npz/...\tvalid\txla\ttile\tinception_v3_batch_128_train_-23e94c034a65a177\ttile:xla\ttile:xla:inception_v3_batch_128_train_-23e94c0...\n",
    "3\t../input/predict-ai-model-runtime/npz_all/npz/...\tvalid\txla\ttile\tinception_v3_batch_128_train_171f4371caf28639\ttile:xla\ttile:xla:inception_v3_batch_128_train_171f4371...\n",
    "4\t../input/predict-ai-model-runtime/npz_all/npz/...\tvalid\txla\ttile\tmlperf_bert_batch_24_2x2_-25e30862c042a2b8\ttile:xla\ttile:xla:mlperf_bert_batch_24_2x2_-25e30862c04...\n",
    "\"\"\"\n",
    "#Dataset\n",
    "#Create an Adjacency matrix for masking the attention\n",
    "#Creates a virtual first node equivalent to the [CLS] token which contains the global config for tile cases, while layout node configuration goes to the corresponding node position\n",
    "def edges_adjacency(edges: torch.Tensor, add_diagonal=True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate an adjacency matrix from the edges\n",
    "    Args:\n",
    "        edges: Tensor of shape (num_edges, 2) with the edges\n",
    "        add_diagonal: Boolean indicating if the diagonal should be added to the adjacency matrix\n",
    "    Returns:\n",
    "        adjacency_matrix: Tensor of shape (num_nodes, num_nodes) with the adjacency matrix\n",
    "    \"\"\"\n",
    "    adjacency_matrix = torch.zeros((edges.max() + 1, edges.max() + 1))\n",
    "    adjacency_matrix[edges[:, 0], edges[:, 1]] = 1\n",
    "    if add_diagonal:\n",
    "        diag_idx = torch.arange(adjacency_matrix.shape[0])\n",
    "        adjacency_matrix[diag_idx, diag_idx] = 1\n",
    "    return adjacency_matrix\n",
    "\n",
    "def tile_loader(path):\n",
    "    tile_dict =  dict(np.load(path))\n",
    "    tile_dict = {k: torch.from_numpy(v) for k, v in tile_dict.items()}\n",
    "    tile_dict['edges_adjecency'] = edges_adjacency(tile_dict['edge_index'])\n",
    "    return tile_dict\n",
    "\n",
    "def node_cls_token(elem_dict, shift_node_config_ids:bool=True):\n",
    "    \"\"\"\n",
    "    Add a cls token to the node opcode, features, edges adjacency matrix, shift node_config_ids by 1 to account for the cls token\n",
    "    Args:\n",
    "        elem_dict: Dictionary with the elements of the tile\n",
    "    Returns:\n",
    "        elem_dict: Dictionary with the elements of the tile with the cls token\n",
    "    \"\"\"\n",
    "    elem_dict['node_opcode'] = torch.cat([torch.tensor([0]), elem_dict['node_opcode']]) # Introduce [CLS] node\n",
    "    elem_dict['node_feat'] = torch.cat([torch.zeros((1, elem_dict['node_feat'].shape[1])), elem_dict['node_feat']])\n",
    "    elem_dict['edges_adjecency'] = F.pad(elem_dict['edges_adjecency'], (1,0,1,0), value=1)\n",
    "    if 'node_config_ids' in elem_dict and shift_node_config_ids:\n",
    "        elem_dict['node_config_ids'] = elem_dict['node_config_ids'] + 1 # Shift Node Config IDs to take in to account [CLS] node\n",
    "    return elem_dict\n",
    "\n",
    "\n",
    "class TileDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df:pd.DataFrame ,add_cls_token:bool=True, num_configs:int=10,  max_configs:Optional[int]=None):\n",
    "        self.df = df\n",
    "        self.add_cls_token = add_cls_token\n",
    "        self.num_configs = num_configs\n",
    "        self.max_configs = max_configs  \n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def select_configs(self, total_configs:int):\n",
    "        if self.max_configs is not None:\n",
    "            total_configs = min(total_configs, self.max_configs)\n",
    "        if self.num_configs == -1:\n",
    "            return np.arange(total_configs)\n",
    "        if total_configs < self.num_configs:\n",
    "            return np.random.choice(total_configs, self.num_configs, replace=True)\n",
    "        return  np.random.choice(total_configs, self.num_configs, replace=False)\n",
    "    \n",
    "    def __getitem__(self, idx:int, selected_configs:List[int]=None):\n",
    "        tile_dict = tile_loader(self.df.paths[idx])\n",
    "        if selected_configs is None:\n",
    "            selected_configs = self.select_configs(tile_dict['config_feat'].shape[0])\n",
    "        tile_dict['node_config_feat'] = tile_dict.pop('config_feat')[selected_configs]\n",
    "        tile_dict['node_config_feat'] = F.pad(tile_dict['node_config_feat'].unsqueeze(1), (0,NODE_CONFIG_FEATS))\n",
    "        tile_dict['config_runtime'] = tile_dict['config_runtime'][selected_configs].float()\n",
    "        tile_dict['config_runtime'] /= tile_dict['config_runtime_normalizers'][selected_configs].float()\n",
    "        tile_dict['node_config_ids'] = torch.zeros((1,))\n",
    "        tile_dict['selected_idxs'] = selected_configs\n",
    "        if self.add_cls_token:\n",
    "            tile_dict = node_cls_token(tile_dict, False)\n",
    "        return tile_dict\n",
    "\"\"\"\n",
    "edges_adjacency関数: 入力: エッジのテンソルと対角線を追加するかどうかのブール値フラグ。 処理: エッジのテンソルから隣接行列を生成します。対角線を追加するオプションもあります。 出力: 隣接行列のテンソル。\n",
    "tile_loader関数: 入力: パス（データセットのエレメントが保存されている場所）。 処理: NumPyファイルをロードし、その内容をPyTorchのテンソルに変換します。さらに、エッジの隣接行列も生成します。 出力: エレメントの辞書（テンソルに変換され、隣接行列も含まれている）。\n",
    "node_cls_token関数: 入力: タイルのエレメントの辞書。 処理: [CLS]トークンをノードのオペコード、特徴、エッジの隣接行列に追加します。また、ノードの設定IDも1だけシフトします。 出力: [CLS]トークンが追加されたタイルのエレメントの辞書。\n",
    "TileDatasetクラス: 役割: タイルのデータセットを管理し、データのロードと前処理を担当します。 initメソッド: インスタンスを初期化し、データフレーム、[CLS]トークンの追加、設定の数などのパラメータを設定します。 lenメソッド: データセットの長さ（エレメントの総数）を返します。 select_configsメソッド: 総設定から特定の設定を選択します。 getitemメソッド: インデックスに基づいてデータセットからエレメントを取得し、必要に応じて前処理を行います。 処理の流れ: グラフのエッジから隣接行列を生成します。 NumPyファイルからタイルデータをロードし、PyTorchのテンソルに変換します。 [CLS]トークンを追加して、ノードやエッジのデータを修正・更新します。 TileDatasetクラスを使用して、データのロードと前処理を効率的に行います。 このコードは、グラフのノードとエッジのデータを効率的に処理して、モデルのトレーニングや評価に使用するためのデータセットを準備するためのものです。\n",
    "\"\"\"\n",
    "\n",
    "tile_dataset = TileDataset(tile_df)\n",
    "elem = tile_dataset[0]\n",
    "for k,v in elem.items():\n",
    "    print(k, v.shape)\n",
    "node_feat torch.Size([81, 140])\n",
    "node_opcode torch.Size([81])\n",
    "edge_index torch.Size([86, 2])\n",
    "config_runtime torch.Size([10])\n",
    "config_runtime_normalizers torch.Size([3246])\n",
    "edges_adjecency torch.Size([81, 81])\n",
    "node_config_feat torch.Size([10, 1, 42])\n",
    "node_config_ids torch.Size([1])\n",
    "selected_idxs (10,)\n",
    "elem['edges_adjecency']\n",
    "\"\"\"\n",
    "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
    "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
    "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
    "        ...,\n",
    "        [1., 0., 0.,  ..., 1., 0., 0.],\n",
    "        [1., 0., 0.,  ..., 0., 1., 0.],\n",
    "        [1., 0., 0.,  ..., 1., 1., 1.]])\n",
    "\"\"\"\n",
    "#Collator\n",
    "def pad_edge_adjacency(edges_adjacency_list):\n",
    "    max_len = max([elem.shape[0] for elem in edges_adjacency_list])\n",
    "    return torch.stack([F.pad(elem, (0, max_len-elem.shape[0], 0, max_len-elem.shape[0]), value=0) for elem in edges_adjacency_list], dim=0)\n",
    "\n",
    "@dataclass\n",
    "class LayoutCollator:\n",
    "    pad_to_multiple_of: int = 64\n",
    "    targets:bool = True\n",
    "    padding_idx:int = 120\n",
    "    node_padding_idx:int = 0\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        output = {}\n",
    "        max_node_len = max([elem['node_opcode'].shape[0] for elem in batch])\n",
    "        node_pad_amount = self.pad_to_multiple_of - max_node_len % max(self.pad_to_multiple_of, 1)\n",
    "        output['node_opcode'] = F.pad(pad_sequence([elem['node_opcode'] for elem in batch], batch_first=True, padding_value=self.padding_idx),\n",
    "                                      (0, node_pad_amount), value=self.padding_idx).long()\n",
    "        output['node_feat'] = F.pad(pad_sequence([elem['node_feat'] for elem in batch], batch_first=True),\n",
    "                                    (0,0,0, node_pad_amount), value=0)\n",
    "        output['edges_adjecency'] = F.pad(pad_edge_adjacency([elem['edges_adjecency'] for elem in batch]),\n",
    "                                          (0, node_pad_amount, 0, node_pad_amount), value=0)\n",
    "        output['node_attn_mask'] = F.pad(pad_sequence([torch.ones(len(elem['node_opcode'])) for elem in batch], batch_first=True),\n",
    "                                         (0, node_pad_amount), value=0)\n",
    "\n",
    "        max_node_config_len = max([elem['node_config_ids'].shape[0] for elem in batch])\n",
    "        node_config_pad_amount = self.pad_to_multiple_of - max_node_config_len % max(self.pad_to_multiple_of, 1)\n",
    "        output['node_config_ids'] = F.pad(pad_sequence([elem['node_config_ids'] for elem in batch], batch_first=True),\n",
    "                                         (0, node_config_pad_amount), value=0).long()\n",
    "        padded_node_config_feat = pad_sequence([elem['node_config_feat'].permute(1,0,2) for elem in batch], batch_first=True, padding_value=-1)\n",
    "        padded_node_config_feat = F.pad(padded_node_config_feat.permute(0,2,1,3),\n",
    "                                           (0,0,0, node_config_pad_amount,0,0), value=-1)\n",
    "        \n",
    "        output['node_config_feat'] = torch.where(padded_node_config_feat!=-1, padded_node_config_feat, self.node_padding_idx)\n",
    "                                      \n",
    "        output['config_idxs'] = torch.stack([torch.from_numpy(elem['selected_idxs']) for elem in batch])\n",
    "        \n",
    "        if self.targets:\n",
    "            output['config_runtime'] = pad_sequence([elem['config_runtime'].float() for elem in batch], batch_first=True)\n",
    "        return output\n",
    "\"\"\"\n",
    "グラフベースのデータセットのバッチ処理を効率的に行うためのデータコレータクラスとその補助関数を定義しています。データコレータは、バッチ内のデータ要素を適切にパディングして、バッチ処理を効率化する役割があります。\n",
    "\n",
    "pad_edge_adjacency関数: 入力: エッジの隣接行列のリスト。 処理: リスト内のすべての隣接行列を、最大の行列サイズに合わせてパディングします。 出力: パディングされた隣接行列のバッチ。\n",
    "LayoutCollatorクラス: 役割: バッチ内のデータを適切にパディングして、一貫した形状のテンソルに変換します。これにより、バッチ処理が効率的に行えます。 pad_to_multiple_of属性: パディングされたデータの長さがこの値の倍数になるように設定します。 targets属性: ターゲット（例: ランタイム）も出力に含めるかどうかを制御します。 padding_idx属性: オペコードのパディングに使用するインデックス値。 node_padding_idx属性: ノードの特徴量のパディングに使用する値。 callメソッド: バッチのデータ処理とパディングを行います。 入力: バッチデータ。 処理: 各データエレメントのノード、エッジ、特徴量などをパディングして、一貫した形状にします。 出力: パディングされ、整形されたバッチデータ。 具体的な処理内容: ノードのオペコードのパディング: バッチ内のすべてのデータエレメントでノードのオペコードを最大の長さにパディングします。 ノードの特徴量のパディング: バッチ内のすべてのデータエレメントでノードの特徴量を最大の長さにパディングします。 エッジの隣接行列のパディング: pad_edge_adjacency関数を使用して、エッジの隣接行列をパディングします。 ノードの設定IDと特徴量のパディング: バッチ内のすべてのデータエレメントでノードの設定IDと特徴量を最大の長さにパディングします。 ランタイムのパディング: ターゲットとして使用するランタイムをパディングします（targets属性がTrueの場合）。 このクラスと関数は、バッチ処理中にデータの形状を一貫させ、モデルのトレーニングや評価を効率的に行うために使用されます。\n",
    "\"\"\"\n",
    "\n",
    "collate_fn = LayoutCollator(64)\n",
    "batch = collate_fn([tile_dataset[0], tile_dataset[1]])\n",
    "for k,v in batch.items():\n",
    "    print(k,v.shape)\n",
    "node_opcode torch.Size([2, 128])\n",
    "node_feat torch.Size([2, 128, 140])\n",
    "edges_adjecency torch.Size([2, 128, 128])\n",
    "node_attn_mask torch.Size([2, 128])\n",
    "node_config_ids torch.Size([2, 64])\n",
    "node_config_feat torch.Size([2, 10, 64, 42])\n",
    "config_idxs torch.Size([2, 10])\n",
    "config_runtime torch.Size([2, 10])\n",
    "Model - Config\n",
    "@dataclass\n",
    "class GraphConfig:\n",
    "    num_hidden_layers: int = 8\n",
    "    hidden_size: int = 256\n",
    "    num_attention_heads: int = 16\n",
    "    intermediate_size: int = 64\n",
    "    chunk_size_feed_forward: int = 64\n",
    "    attention_probs_dropout_prob: float = 0.0\n",
    "    max_position_embeddings: int = 512\n",
    "    hidden_dropout_prob: float = 0.0\n",
    "    layer_norm_eps: float = 1e-12\n",
    "    hidden_act: str = 'gelu'\n",
    "    initializer_range: float = 0.02\n",
    "    output_hidden_states: bool = False\n",
    "    output_attentions: bool = False\n",
    "    gradient_checkpointing: bool = False\n",
    "    margin: float = 0.1\n",
    "    number_permutations: int = 10\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.embedding_size = self.hidden_size\n",
    "    \n",
    "    def validate(self):\n",
    "        if self.hidden_size % self.num_attention_heads != 0 and not hasattr(self, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({self.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({self.num_attention_heads})\"\n",
    "            )\n",
    "            \n",
    "    def save_config(self, path):\n",
    "        config = asdict(self)\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(config, f)\n",
    "            \n",
    "    @classmethod\n",
    "    def load_config(cls, path):\n",
    "        with open(path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        return cls(**config)\n",
    "\n",
    "\"\"\"\n",
    "グラフネットワークまたはトランスフォーマーモデルの設定を管理するGraphConfigクラスを定義しています。このクラスは、モデルのハイパーパラメータと設定を格納、検証、保存、ロードする機能を提供します。\n",
    "\n",
    "GraphConfig クラスの属性： num_hidden_layers: モデルの隠れ層の数。 hidden_size: 隠れ層のユニット数（次元数）。 num_attention_heads: アテンションヘッドの数。 intermediate_size: インターメディエイトレイヤー（フィードフォワードネットワーク部分）のサイズ。 chunk_size_feed_forward: フィードフォワードネットワークのチャンクサイズ。 attention_probs_dropout_prob: アテンション確率のドロップアウト率。 max_position_embeddings: 最大位置埋め込みのサイズ。 hidden_dropout_prob: 隠れ層のドロップアウト率。 layer_norm_eps: レイヤー正規化のepsilon（安定性のための小さい値）。 hidden_act: 隠れ層の活性化関数。 initializer_range: 重みの初期化の範囲。 output_hidden_states: 隠れ状態を出力するかどうか。 output_attentions: アテンションを出力するかどうか。 gradient_checkpointing: 勾配のチェックポイントを使用するかどうか（メモリ効率のため）。 margin: ロス計算で使用するマージン。 number_permutations: パーミュテーションの数。\n",
    "\n",
    "GraphConfig クラスのメソッド： post_init メソッド: インスタンスの初期化が完了した後に、embedding_size を hidden_size と同じ値で設定します。 validate メソッド: モデルの設定が正しいことを確認します。特に、hidden_size が num_attention_heads の倍数であることを確認します。 save_config メソッド: モデルの設定をJSONファイルとして保存します。 入力: 設定を保存するパス。 処理: 設定ディクショナリをJSONとしてファイルに書き込みます。 load_config クラスメソッド: JSONファイルからモデルの設定をロードします。 入力: 設定ファイルのパス。 出力: ロードされた設定を持つ新しい GraphConfig インスタンス。\n",
    "\n",
    "Loss\n",
    "Uses Ranking loss to compare different configuration\n",
    "Compares does configurations with different indexes, masks those cases where the permutation returns the same element\n",
    "Compares multiple configurations in each run\n",
    "\"\"\"\n",
    "class MultiElementRankLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss function that compares the output of the model with the output of the model with a permutation of the elements\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, margin:float=0.0, number_permutations:int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.loss_fn = torch.nn.MarginRankingLoss(margin=margin, reduction = 'none')\n",
    "        self.number_permutations = number_permutations\n",
    "    \n",
    "    def calculate_rank_loss(self,\n",
    "                            outputs: torch.Tensor,\n",
    "                            config_runtime: torch.Tensor,\n",
    "                            config_idxs: torch.Tensor\n",
    "                            ):\n",
    "        \"\"\"\n",
    "        Generates a permutation of the predictions and targets and calculates the loss MarginRankingLoss against the permutation\n",
    "        Args:\n",
    "            outputs: Tensor of shape (bs, seq_len) with the outputs of the model\n",
    "            config_runtime: Tensor of shape (bs, seq_len) with the runtime of the model\n",
    "            config_mask: Tensor of shape (bs, seq_len) with 1 in the positions of the elements\n",
    "            and 0 in the positions of the padding\n",
    "        Returns:\n",
    "            loss: Tensor of shape (bs, seq_len) with the loss for each element in the batch\n",
    "        \"\"\"\n",
    "        bs, num_configs = outputs.shape\n",
    "        permutation = torch.randperm(num_configs) \n",
    "        permuted_idxs = config_idxs[:, permutation]\n",
    "        # We mask those cases where we compare the same configuration\n",
    "        config_mask = torch.where(config_idxs != permuted_idxs, 1, 0)\n",
    "        permuted_runtime = config_runtime[:, permutation]\n",
    "        labels = 2*((config_runtime - permuted_runtime) > 0) -1\n",
    "        permuted_output = outputs[:, permutation]\n",
    "        loss = self.loss_fn(outputs.view(-1,1), permuted_output.view(-1,1), labels.view(-1,1))\n",
    "        loss = loss.view(bs, num_configs) * config_mask\n",
    "        return loss.mean()\n",
    "                \n",
    "    \n",
    "    def forward(self,\n",
    "                outputs: torch.Tensor,\n",
    "                config_runtime: torch.Tensor,\n",
    "                config_idxs: torch.Tensor\n",
    "                ):\n",
    "        loss = 0 \n",
    "        for _ in range(self.number_permutations):\n",
    "            loss += self.calculate_rank_loss(outputs, config_runtime, config_idxs)\n",
    "        return loss/ self.number_permutations\n",
    "\"\"\"\n",
    "このコードは、複数のエレメントのランキング損失（MultiElementRankLoss）を計算するPyTorchのカスタム損失関数クラスを定義しています。この損失関数は、モデルの出力とエレメントの順序を変更したモデルの出力を比較し、ランキングが正しくない場合にペナルティを与えます。\n",
    "\n",
    "クラスとメソッドの詳細：\n",
    "\n",
    "MultiElementRankLoss クラス:\n",
    "目的: モデルの出力とエレメントの順序を変更したモデルの出力を比較して損失を計算する。\n",
    "\n",
    "init メソッド:\n",
    "入力: margin と number_permutations。 処理: MarginRankingLoss を初期化し、パラメータを設定する。\n",
    "\n",
    "calculate_rank_loss メソッド:\n",
    "入力: outputs, config_runtime, config_idxs。 outputs: モデルの出力。 config_runtime: 各エレメントのランタイム。 config_idxs: 各エレメントのインデックス。 処理: エレメントの順序をランダムに並べ替えて、MarginRankingLoss を計算する。この並べ替えはランキングの精度を評価するために行われます。 出力: 各バッチエレメントの平均損失。\n",
    "\n",
    "入力: outputs, config_runtime, config_idxs。 処理: 指定された数の順列で calculate_rank_loss を繰り返し、平均損失を計算する。 出力: 最終的な平均損失。 クラスの動作: インスタンス化: margin と number_permutations を指定してクラスをインスタンス化します。margin はマージンランキング損失のマージン、number_permutations は順列の数です。 損失計算: forward メソッドを呼び出して損失を計算します。これにはモデルの出力、設定のランタイム、設定のインデックスが必要です。 順列と損失: calculate_rank_loss メソッドは、モデルの出力とその順列を使ってマージンランキング損失を計算します。同じ設定で比較されるケースはマスクされ、損失の計算には含まれません。 平均損失: すべての順列に対する損失の平均が最終的な損失として返されます。 このカスタム損失関数は、モデルがグラフのエレメントを正しくランク付けする能力を評価と最適化するために使われます。\n",
    "\n",
    "\"\"\"\n",
    "#Metric\n",
    "class TileTopK(tm.Metric):\n",
    "    \n",
    "    higher_is_better = True\n",
    "    \n",
    "    def __init__(self, k:int=5) -> None:\n",
    "        super().__init__()\n",
    "        self.add_state(\"runtimes\", default=[], dist_reduce_fx=None)\n",
    "        self.k = k\n",
    "        \n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor, config_attn_mask:torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Update the metric state\n",
    "        Args:\n",
    "            preds: Tensor of shape (bs, seq_len) with the predicted runtimes orders\n",
    "            target: Tensor of shape (bs, seq_len) with the target runtimes\n",
    "            config_attn_mask: Tensor of shape (bs, seq_len) with 1 in the positions of the elements\n",
    "        \"\"\"\n",
    "        best_runtimes = torch.where(config_attn_mask==1, target, torch.tensor(float('inf'))).min(1).values\n",
    "        masked_preds = torch.where(config_attn_mask==1, preds, torch.tensor(float('inf')))\n",
    "        pred_bottomk_indices = torch.topk(masked_preds, k=self.k, largest=False).indices\n",
    "        bs = preds.shape[0]\n",
    "        bottom_k_positions = torch.stack([torch.arange(bs).repeat_interleave(self.k).to(config_attn_mask.device), pred_bottomk_indices.view(-1)])\n",
    "        predicted_runtimes = target[bottom_k_positions[0], bottom_k_positions[1]].view(bs,self.k)\n",
    "        best_predicted_runtimes = predicted_runtimes.min(1).values\n",
    "        self.runtimes.append(best_predicted_runtimes/ best_runtimes)\n",
    "        \n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return (2-torch.cat(self.runtimes)).mean()\n",
    "\"\"\"\n",
    "モデルが予測したランタイムとターゲットランタイムを比較して、ランタイムの予測の正確さを評価するためのカスタムメトリクスを定義しています。具体的には、Top-K メトリクスを使用して、モデルが予測したトップKの設定のランタイムと、実際の最適なランタイムとを比較します。\n",
    "\n",
    "TileTopK クラスの説明：\n",
    "\n",
    "属性とメソッド:\n",
    "higher_is_better: このメトリクスが高いほど良い、という意味でTrueに設定されています。 init メソッド: 初期化メソッドで、Top-KのKの値を設定します。 update メソッド: 予測とターゲットのランタイム、および設定のアテンションマスクを取得して、メトリクスの状態を更新します。 compute メソッド: 現在のメトリクスの状態から最終的なメトリクスの値を計算します。\n",
    "\n",
    "update メソッドの動作:\n",
    "入力: preds: モデルによって予測されたランタイムの順序。 target: 実際のランタイムの目標値。 config_attn_mask: エレメントの位置に1、パディングの位置に0が入ったアテンションマスク。\n",
    "\n",
    "処理: アテンションマスクを使用して、有効な設定の位置のランタイムの最小値（最適なランタイム）を計算します。 予測されたランタイムのうち、Top-Kの設定を選択します。 選択されたTop-K設定の予測ランタイムと実際の最適なランタイムを比較します。 この比較の結果を状態に保存します。\n",
    "\n",
    "compute メソッドの動作:\n",
    "処理: update メソッドで保存されたすべての比較結果から、最終的なメトリクスの値を計算します。 出力: 計算されたメトリクスの値を返します。 使い方のシナリオ： モデルの訓練または評価中に、このTileTopKメトリクスを使用して、モデルのランタイム予測の性能を評価します。 それぞれのバッチで、updateメソッドを呼び出して、予測とターゲットのランタイム、および設定のアテンションマスクを渡します。これにより、メトリクスの状態が更新されます。 評価が完了したら、computeメソッドを呼び出して、最終的なメトリクスの値を取得します。これがモデルのランタイム予測の性能を示す値になります。 このメトリクスは、モデルがどれだけ正確にランタイムを予測できるか、特にトップKの設定に焦点を当てて評価するのに役立ちます。\n",
    "\"\"\"\n",
    "#Model\n",
    "#Modified version of 🤗 Bert implementation to take in to account Graph Attention\n",
    "#\n",
    "#Removed the parts corresponding to Cross-attention\n",
    "#Made layer_head_mask the same for all layers, heads\n",
    "#The Head mask corresponds to the edge adjacency\n",
    "# Modified from https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config:GraphConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        output_hidden_states: Optional[bool] = False,\n",
    "        return_dict: Optional[bool] = True,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask #DONE: Same Head Mask for all layers\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs,  output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=None,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=None,\n",
    "        )\n",
    "        \n",
    "        \n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config:GraphConfig):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "    \n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config:GraphConfig):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "    \n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config:GraphConfig):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "    \n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config:GraphConfig, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        self_outputs = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config:GraphConfig, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = position_embedding_type or getattr(\n",
    "            config, \"position_embedding_type\", \"absolute\"\n",
    "        )\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "\n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        \n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n",
    "            position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask #DONE: Same Head Mask for all Heads\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config:GraphConfig):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "    \n",
    "    \n",
    "class NodeEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, config:GraphConfig):\n",
    "        super().__init__()\n",
    "        self.node_opcode_embeddings = nn.Embedding(NODE_OP_CODES+1 , config.embedding_size, padding_idx=NODE_OP_CODES)\n",
    "        self.linear = nn.Linear(NODE_FEATS, config.embedding_size, bias=False)\n",
    "        self.layer_norm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "        \n",
    "    def forward(self,\n",
    "                node_opcode: torch.Tensor,\n",
    "                node_feat: torch.Tensor\n",
    "                ) -> torch.Tensor:\n",
    "        opcode_embeddings = self.node_opcode_embeddings(node_opcode) \n",
    "        node_feats =  self.linear(node_feat)\n",
    "        features = opcode_embeddings + node_feats\n",
    "        features = self.layer_norm(features)\n",
    "        return features\n",
    "    \n",
    "    \n",
    "class BertNodeEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, config:GraphConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.node_embeddings = NodeEncoder(config)\n",
    "        self.node_encoder = BertEncoder(config)\n",
    "        \n",
    "    def forward(self,\n",
    "                node_opcode: torch.Tensor,\n",
    "                node_feat: torch.Tensor,\n",
    "                edges_adjecency: torch.Tensor,\n",
    "                node_attn_mask: torch.Tensor\n",
    "                ):\n",
    "        node_embeddings = self.node_embeddings(node_opcode, node_feat)\n",
    "        node_attn_mask = node_attn_mask.unsqueeze(1).unsqueeze(-1)\n",
    "        node_encoder_outputs = self.node_encoder(node_embeddings,\n",
    "                                                 attention_mask=node_attn_mask,\n",
    "                                                 head_mask=edges_adjecency.unsqueeze(0).repeat(self.config.num_hidden_layers, 1, 1, 1).unsqueeze(2),\n",
    "                                                 output_attentions=True)\n",
    "        return node_encoder_outputs\n",
    "    \n",
    "def transform_node_positional_embeddings(embeddings_output:torch.Tensor,\n",
    "                                         node_config_ids:torch.Tensor,\n",
    "                                         num_nodes:int\n",
    "                                         ) -> torch.Tensor:\n",
    "    bs, num_configs, _, dim = embeddings_output.shape\n",
    "    idxs = node_config_ids.unsqueeze(1).repeat(1,num_configs,1)\n",
    "    zeros = torch.zeros(bs, num_configs, num_nodes, dim, device=embeddings_output.device, dtype=embeddings_output.dtype)\n",
    "    idxs = idxs.unsqueeze(-1).repeat(1,1,1,dim)\n",
    "    zeros.scatter_reduce_(2, idxs, embeddings_output, reduce='sum')\n",
    "    return zeros\n",
    "\n",
    "class NodeFeatEmbeddings(nn.Module):\n",
    "    def __init__(self, config:GraphConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.node_feat_embeddings = nn.Linear(NODE_CONFIG_FEATS + CONFIG_FEATS, config.embedding_size, bias=False)\n",
    "        self.layer_norm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "    def forward(self, node_config_feat: torch.Tensor, node_config_ids: torch.Tensor, num_nodes:int) -> torch.Tensor:\n",
    "        node_config_feat_embeddings = self.node_feat_embeddings(node_config_feat)\n",
    "        node_config_feat_embeddings = self.layer_norm(node_config_feat_embeddings)\n",
    "        node_config_feat_embeddings = transform_node_positional_embeddings(node_config_feat_embeddings, node_config_ids, num_nodes)\n",
    "        return node_config_feat_embeddings\n",
    "        \n",
    "    \n",
    "class BertGraphEncoder(nn.Module):\n",
    "    def __init__(self, config:GraphConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.node_embeddings = NodeEncoder(config)\n",
    "        self.node_encoder = BertEncoder(config)\n",
    "        self.node_feat_embeddings = NodeFeatEmbeddings(config)\n",
    "        \n",
    "    def forward(self,\n",
    "                node_opcode: torch.Tensor, # (bs, num_nodes)\n",
    "                node_feat: torch.Tensor, # (bs, num_nodes, num_node_feats)\n",
    "                edges_adjecency: torch.Tensor, # (bs, num_nodes, num_nodes)\n",
    "                node_attn_mask: torch.Tensor, # (bs, num_nodes)\n",
    "                node_config_feat: torch.Tensor, # (bs, num_configs, num_config_nodes, num_node_feats)\n",
    "                node_config_ids: torch.Tensor, # (bs, num_configs, num_config_nodes)\n",
    "                ):\n",
    "        bs, num_nodes = node_opcode.shape\n",
    "        num_configs = node_config_feat.shape[1]\n",
    "        node_embeddings = self.node_embeddings(node_opcode, node_feat)\n",
    "        node_config_feat_embeddings = self.node_feat_embeddings(node_config_feat, node_config_ids, num_nodes)\n",
    "        \n",
    "        node_embeddings = node_embeddings.unsqueeze(1).repeat(1, num_configs, 1, 1)\n",
    "        node_embeddings += node_config_feat_embeddings\n",
    "        node_attn_mask = node_attn_mask.unsqueeze(1).repeat(1, num_configs, 1)\n",
    "        node_embeddings = node_embeddings.reshape(bs *num_configs, num_nodes, -1)\n",
    "        node_attn_mask = node_attn_mask.reshape(bs *num_configs, num_nodes)\n",
    "        node_attn_mask = node_attn_mask.unsqueeze(1).unsqueeze(-1)\n",
    "        edges_adjecency = edges_adjecency.unsqueeze(1).repeat(1, num_configs, 1, 1).reshape(bs *num_configs, num_nodes, num_nodes)\n",
    "        edges_adjecency = edges_adjecency.unsqueeze(1)\n",
    "        \n",
    "\n",
    "        node_encoder_outputs = self.node_encoder(node_embeddings,\n",
    "                                                 attention_mask=node_attn_mask,\n",
    "                                                 head_mask=edges_adjecency,\n",
    "                                                 output_attentions=True)\n",
    "        \n",
    "        return node_encoder_outputs.last_hidden_state.reshape(bs, num_configs, num_nodes, -1)\n",
    "    \n",
    "    \n",
    "class GraphEncoder(nn.Module):\n",
    "    \n",
    "    config_class = GraphConfig\n",
    "    \n",
    "    def __init__(self, config:GraphConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.node_encoder = BertGraphEncoder(config)\n",
    "        self.head = nn.Linear(config.hidden_size, 1)\n",
    "        self.loss_fn = MultiElementRankLoss(margin=config.margin, number_permutations=config.number_permutations)\n",
    "        \n",
    "        \n",
    "    def forward(self,\n",
    "                node_opcode: torch.Tensor, # (bs, num_nodes)\n",
    "                node_feat: torch.Tensor, # (bs, num_nodes, num_node_feats)\n",
    "                edges_adjecency: torch.Tensor, # (bs, num_nodes, num_nodes)\n",
    "                node_attn_mask: torch.Tensor, # (bs, num_nodes)\n",
    "                node_config_feat: torch.Tensor, # (bs, num_configs, num_config_nodes, num_node_feats)\n",
    "                node_config_ids: torch.Tensor, # (bs, num_configs, num_config_nodes)\n",
    "                config_idxs: Optional[torch.Tensor] = None, # (bs, num_configs)\n",
    "                config_runtime: Optional[torch.Tensor] = None,):\n",
    "        \n",
    "        last_hidden_state = self.node_encoder(node_opcode,\n",
    "                                    node_feat,\n",
    "                                    edges_adjecency,\n",
    "                                    node_attn_mask,\n",
    "                                    node_config_feat,\n",
    "                                    node_config_ids)\n",
    "        \n",
    "        output = self.head(last_hidden_state[:,:,0]).squeeze(-1)\n",
    "        outputs = {'outputs': output, 'order': torch.argsort(output, dim=1)}\n",
    "        if config_runtime is not None:\n",
    "            loss = 0\n",
    "            loss += self.loss_fn(output, config_runtime, config_idxs)\n",
    "            outputs['loss'] = loss\n",
    "        return outputs\n",
    "\"\"\"\n",
    "グラフのエンコーディングにBERTモデルのアーキテクチャを利用するカスタムニューラルネットワークモデルを定義しています。ノードとエッジの情報を処理して特徴量を抽出し、それを利用して特定のタスク（ここでは設定のランタイムを予測する）のための出力を生成します。コードは複数のクラスとメソッドで構成され、大量の情報を含んでいるため、以下に主要な部分を分解して説明します。\n",
    "\n",
    "BertEncoder クラス 目的: BERTモデルのエンコーダ部分を定義しています。エンコーダは、入力テンソル（ノードとエッジの情報）を特徴ベクトルに変換する役割を果たします。 主要メソッド: forward は、エンコーディングの処理を行い、特徴ベクトル、注意の重み、隠れ状態などを返します。\n",
    "BertLayer およびその関連クラス (BertIntermediate, BertOutput, BertAttention, BertSelfAttention, BertSelfOutput) 目的: BERTの内部層を定義しています。それぞれのクラスは、BERTのアテンションメカニズム、フィードフォワードネットワーク、正規化、ドロップアウトなどのコンポーネントを表現しています。 主要メソッド: 各クラスは forward メソッドを持っており、それぞれの部分の処理を行います。\n",
    "NodeEncoder クラス 目的: グラフのノード情報をエンコードするためのクラスです。ノードのオペコードと特徴をエンコードして、それぞれのノードの特徴ベクトルを生成します。 主要メソッド: forward メソッドは、ノードのオペコードと特徴を受け取り、それをエンコードして特徴ベクトルを返します。\n",
    "BertNodeEncoder クラス 目的: NodeEncoder と BertEncoder を組み合わせて、グラフのノードをエンコードするクラスです。 主要メソッド: forward メソッドは、ノードとエッジの情報を受け取り、それをエンコードして特徴ベクトルと注意の重みを返します。\n",
    "transform_node_positional_embeddings 関数 この関数は、ノードの位置エンベッディングを変換する役割を果たしています。\n",
    "パラメータ: embeddings_output: エンコーダからの出力エンベッディング。 node_config_ids: ノードの設定ID。 num_nodes: ノードの合計数。 動作: bs, numconfigs, , dim 変数は、エンベッディングの出力形状から得られます。bs はバッチサイズ、num_configs は設定の数、dim はエンベッディングの次元です。 idxs 変数は、ノード設定IDを用いて、各ノードの位置エンベッディングを取得するためのインデックスを生成します。 zeros は、変換後のエンベッディングを格納するためのゼロテンソルです。 最後に scatterreduce メソッドを用いて、embeddings_output からノードの位置エンベッディングを取得し、zeros テンソルに格納します。\n",
    "\n",
    "NodeFeatEmbeddings クラス このクラスは、ノードとその特徴をエンコードし、位置エンベッディングを含む特徴ベクトルを生成します。\n",
    "主要メソッド: forward メソッドは、ノードの設定特徴とIDを受け取り、それをエンコードして特徴ベクトルを生成します。transform_node_positional_embeddings 関数を使用して、位置エンベッディングを取得しています。\n",
    "\n",
    "BertGraphEncoder クラス このクラスは、グラフのエンコーディング全体を担当します。ノードとエッジの情報、ノードの位置エンベッディングなどを処理し、グラフの特徴ベクトルを生成します。\n",
    "主要メソッド: forward メソッドは、ノードとエッジの情報、ノードの設定情報などを受け取り、それをエンコードして特徴ベクトルを生成します。このクラスでは、ノードのエンコーディングと位置エンベッディングの両方を処理しています。\n",
    "\n",
    "GraphEncoder クラス これは、グラフのエンコーディングと、そのエンコーディングを基にしたランタイムの予測を行うためのメインクラスです。\n",
    "主要メソッド: forward メソッドは、グラフのノードとエッジの情報、ノードの設定情報、ランタイムなどを受け取り、エンコーディングとランタイムの予測を行います。また、必要に応じて、損失も計算します。 このクラスは、先に説明したBertGraphEncoderクラスを使用して、グラフをエンコードしています。さらに、MultiElementRankLoss損失関数を使用して、ランタイムの予測のための損失を計算します。\n",
    "\"\"\"\n",
    "\n",
    "class LightningWrapper(pl.LightningModule):\n",
    "    def __init__(self, model:nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.topk = TileTopK()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.model(**batch)\n",
    "        return outputs['loss']\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.model(**batch)\n",
    "        loss = outputs['loss']\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        config_attn_mask = torch.ones_like(batch['config_runtime'], device=batch['config_runtime'].device)\n",
    "        self.topk.update(outputs['outputs'], batch['config_runtime'], config_attn_mask)\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_end(self) -> None:\n",
    "        topk = self.topk.compute()\n",
    "        self.print(f\"topk {topk:.3f}\")\n",
    "        self.topk.reset()\n",
    "        return super().on_validation_end()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.model.loss(y_hat, y)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.trainer.model.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\"\"\"\n",
    "LightningWrapper クラスの構造\n",
    "\n",
    "init メソッド\n",
    "このメソッドはクラスの初期化メソッドです。モデルとTop-K評価メトリックをインスタンス変数として保持します。\n",
    "\n",
    "model: このインスタンス変数は、訓練、検証、テストを行うためのモデルを保持します。 topk: このインスタンス変数は、検証の際にモデルのパフォーマンスを評価するためのTop-Kメトリックを保持します。\n",
    "\n",
    "forward メソッド\n",
    "このメソッドは、モデルのフォワードパスを呼び出します。具体的には、入力xをモデルに渡して、出力を返します。\n",
    "\n",
    "training_step メソッド\n",
    "このメソッドは、各訓練ステップで呼び出され、バッチデータに対する訓練の損失を計算します。バッチデータはbatch引数として渡され、損失はモデルの出力から取得します。\n",
    "\n",
    "validation_step メソッド\n",
    "このメソッドは、各検証ステップで呼び出され、バッチデータに対する検証の損失を計算します。さらに、Top-Kメトリックもこのステップで計算され、更新されます。\n",
    "\n",
    "loss: 検証の損失を計算します。 self.log: 検証の損失をログに記録します。 config_attn_mask: 設定の注目マスクを作成します。 self.topk.update: Top-Kメトリックを更新します。\n",
    "\n",
    "on_validation_end メソッド\n",
    "検証が終了したときに呼び出され、Top-Kメトリックの値を計算して表示します。その後、Top-Kメトリックをリセットします。\n",
    "\n",
    "test_step メソッド\n",
    "テストステップで呼び出され、バッチデータに対するテストの損失を計算します。\n",
    "\n",
    "configure_optimizers メソッド\n",
    "オプティマイザを設定するメソッドです。この例では、AdamWオプティマイザを使用しています。学習率は0.001に設定されています。\n",
    "\n",
    "LightningWrapper クラスは、グラフエンコーダモデルをPyTorch Lightningフレームワークを使って効率的に訓練、検証、テストするためのユーティリティクラスです。訓練と検証のステップで損失を計算し、検証のステップでパフォーマンスメトリックも計算します。また、オプティマイザの設定やテストステップの実装も含まれています。\n",
    "\"\"\"\n",
    "#Training\n",
    "\n",
    "config_kwargs = dict(hidden_size= 128,\n",
    "    num_attention_heads= 4,\n",
    "    num_hidden_layers= 2,\n",
    "    intermediate_size= 64,\n",
    "    gradient_checkpointing= True,\n",
    "    margin= 0.1,\n",
    "    number_permutations= 4,\n",
    "    )\n",
    "config = GraphConfig(**config_kwargs)\n",
    "model = GraphEncoder(config)\n",
    "model = LightningWrapper(model)\n",
    "tile_df\n",
    "\"\"\"\n",
    "paths\tsplit\tconfiguration\textra\tmodel_name\tcollection\tID\n",
    "0\t../input/predict-ai-model-runtime/npz_all/npz/...\tvalid\txla\ttile\tresnet_v1_50_official_batch_128_bf16_2bea628b7...\ttile:xla\ttile:xla:resnet_v1_50_official_batch_128_bf16_...\n",
    "1\t../input/predict-ai-model-runtime/npz_all/npz/...\tvalid\txla\ttile\tinception_v3_batch_128_train_40fa8f86f121f00a\ttile:xla\ttile:xla:inception_v3_batch_128_train_40fa8f86...\n",
    "2\t../input/predict-ai-model-runtime/npz_all/npz/...\tvalid\txla\ttile\tinception_v3_batch_128_train_-23e94c034a65a177\ttile:xla\ttile:xla:inception_v3_batch_128_train_-23e94c0...\n",
    "3\t../input/predict-ai-model-runtime/npz_all/npz/...\tvalid\txla\ttile\tinception_v3_batch_128_train_171f4371caf28639\ttile:xla\ttile:xla:inception_v3_batch_128_train_171f4371...\n",
    "4\t../input/predict-ai-model-runtime/npz_all/npz/...\tvalid\txla\ttile\tmlperf_bert_batch_24_2x2_-25e30862c042a2b8\ttile:xla\ttile:xla:mlperf_bert_batch_24_2x2_-25e30862c04...\n",
    "...\t...\t...\t...\t...\t...\t...\t...\n",
    "7224\t../input/predict-ai-model-runtime/npz_all/npz/...\ttrain\txla\ttile\tshapemask.4x4.fp32_-308d824e29eea7d5\ttile:xla\ttile:xla:shapemask.4x4.fp32_-308d824e29eea7d5\n",
    "7225\t../input/predict-ai-model-runtime/npz_all/npz/...\ttrain\txla\ttile\tmnasnet_b1_batch_128_274248815373b90a\ttile:xla\ttile:xla:mnasnet_b1_batch_128_274248815373b90a\n",
    "7226\t../input/predict-ai-model-runtime/npz_all/npz/...\ttrain\txla\ttile\tshapemask.4x4.fp32_15c8ed14770f4c5e\ttile:xla\ttile:xla:shapemask.4x4.fp32_15c8ed14770f4c5e\n",
    "7227\t../input/predict-ai-model-runtime/npz_all/npz/...\ttrain\txla\ttile\tinception_v2_batch_8_train_-2780d93f2933627\ttile:xla\ttile:xla:inception_v2_batch_8_train_-2780d93f2...\n",
    "7228\t../input/predict-ai-model-runtime/npz_all/npz/...\ttrain\txla\ttile\tretinanet.4x4.fp32_-5ad42689cc8da2aa\ttile:xla\ttile:xla:retinanet.4x4.fp32_-5ad42689cc8da2aa\n",
    "7229 rows × 7 columns\n",
    "\"\"\"\n",
    "train_df = tile_df.query(\"split == 'train'\").reset_index(drop=True)\n",
    "valid_df = tile_df.query(\"split == 'valid'\").reset_index(drop=True)\n",
    "train_dataset = TileDataset(train_df, num_configs=24)\n",
    "valid_dataset = TileDataset(valid_df, num_configs=24)\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=8, num_workers=2, shuffle=True, persistent_workers=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, collate_fn=collate_fn, batch_size=8, num_workers=2)\n",
    "trainer_config = dict(\n",
    "    max_epochs= 50,\n",
    "    precision= 32,\n",
    "    gradient_clip_val= 1.0,\n",
    "    accumulate_grad_batches= 4,\n",
    "    check_val_every_n_epoch= 10)\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "trainer = pl.Trainer(**trainer_config,)\n",
    "trainer.fit(model, train_dataloader, valid_dataloader)\n",
    "\"\"\"\n",
    "Epoch 49: 100%\n",
    "714/714 [00:33<00:00, 21.61it/s, v_num=0, val_loss=0.0322]\n",
    "topk 0.982\n",
    "topk 0.983\n",
    "topk 0.986\n",
    "topk 0.987\n",
    "topk 0.991\n",
    "\"\"\"\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "split = 'test'\n",
    "test_tile_df = tile_df.query(\"split == @split\").reset_index(drop=True)\n",
    "test_tile_ds = TileDataset(test_tile_df, num_configs=-1)\n",
    "collate_fn = LayoutCollator(64, targets=split!=\"test\")\n",
    "test_dataloader = DataLoader(test_tile_ds, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "model.to(device)\n",
    "model = model.eval()\n",
    "def chunk_batch(batch, start_idx, end_idx):\n",
    "    output = {k:batch[k] for k in ['node_opcode', 'node_feat', 'edges_adjecency', 'node_attn_mask', 'node_config_ids']}\n",
    "    output['node_config_feat'] = batch['node_config_feat'][:, start_idx: end_idx]\n",
    "    return output\n",
    "    \n",
    "pred_order = []\n",
    "for batch in tqdm(test_dataloader):\n",
    "    batch.pop('config_idxs')\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    num_configs = batch['node_config_feat'].shape[1]\n",
    "    # Chunk the configs to avoid OOM errors\n",
    "    configs_cut_points = list(range(0,num_configs, 100)) + [num_configs]\n",
    "    chunk_order = []\n",
    "    for start, end in zip(configs_cut_points, configs_cut_points[1:]):\n",
    "        chunked_batch = chunk_batch(batch, start, end)\n",
    "        with torch.no_grad():\n",
    "            output = model.model(**chunked_batch)\n",
    "        chunk_order.extend(output['outputs'].cpu().numpy())\n",
    "    pred_order.append(np.argsort(np.concatenate(chunk_order))[:5])\n",
    "\"\"\"\n",
    "100%|██████████| 844/844 [01:18<00:00, 10.81it/s]\n",
    "\"\"\"\n",
    "idxs_string = [\";\".join(map(str,elem)) for elem in pred_order]\n",
    "test_tile_df['TopConfigs'] = idxs_string\n",
    "test_tile_df = test_tile_df[['ID', 'TopConfigs']]\n",
    "test_tile_df.head()\n",
    "\"\"\"\n",
    "ID\tTopConfigs\n",
    "0\ttile:xla:04ae9238c653f8ae08f60f2c03615f0b\t273;299;479;661;385\n",
    "1\ttile:xla:85d157d3b1848c6b6fff0c633876e2e6\t6792;8019;7513;2902;3531\n",
    "2\ttile:xla:862900d42397d03be2762e1bf7518bea\t206;161;1409;287;1344\n",
    "3\ttile:xla:0afa527a7022415fda1dd69d11e908a4\t158;210;212;69;20\n",
    "4\ttile:xla:2d09e3ab92e184c561abaf8d9efe7b87\t170;147;24;89;6\n",
    "\"\"\"\n",
    "submission_df = pd.read_csv('../input/predict-ai-model-runtime/sample_submission.csv')\n",
    "submission_df = submission_df.query(f\"ID not in {test_tile_df.ID.tolist()}\")\n",
    "submission_df = pd.concat([test_tile_df, submission_df])\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "submission_df\n",
    "\"\"\"\n",
    "ID\tTopConfigs\n",
    "0\ttile:xla:04ae9238c653f8ae08f60f2c03615f0b\t273;299;479;661;385\n",
    "1\ttile:xla:85d157d3b1848c6b6fff0c633876e2e6\t6792;8019;7513;2902;3531\n",
    "2\ttile:xla:862900d42397d03be2762e1bf7518bea\t206;161;1409;287;1344\n",
    "3\ttile:xla:0afa527a7022415fda1dd69d11e908a4\t158;210;212;69;20\n",
    "4\ttile:xla:2d09e3ab92e184c561abaf8d9efe7b87\t170;147;24;89;6\n",
    "...\t...\t...\n",
    "889\tlayout:nlp:random:60880ed76de53f4d7a1b960b24f2...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "890\tlayout:nlp:random:23559853d9702baaaacbb0c83fd3...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "891\tlayout:nlp:random:f6c146fc5cf10be4f3accbaca989...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "892\tlayout:nlp:random:32531d07a084b319dce484f53a4c...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "893\tlayout:nlp:random:3a0c5517a87df8d82fd637b83298...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "894 rows × 2 columns\n",
    "\"\"\"\n",
    "\n",
    "!pip install /kaggle/input/fast-slow-4-dataset-train/torch_geometric-2.3.1-py3-none-any.whl\n",
    "!pip install /kaggle/input/fast-slow-4-dataset-train/torch_scatter-2.1.1-cp310-cp310-linux_x86_64.whl\n",
    "\"\"\"\n",
    "Processing /kaggle/input/fast-slow-4-dataset-train/torch_geometric-2.3.1-py3-none-any.whl\n",
    "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.3.1) (4.66.1)\n",
    "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.3.1) (1.23.5)\n",
    "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.3.1) (1.11.2)\n",
    "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.3.1) (3.1.2)\n",
    "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.3.1) (2.31.0)\n",
    "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.3.1) (3.0.9)\n",
    "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.3.1) (1.2.2)\n",
    "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.3.1) (5.9.3)\n",
    "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric==2.3.1) (2.1.3)\n",
    "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric==2.3.1) (3.1.0)\n",
    "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric==2.3.1) (3.4)\n",
    "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric==2.3.1) (1.26.15)\n",
    "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric==2.3.1) (2023.7.22)\n",
    "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric==2.3.1) (1.3.2)\n",
    "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric==2.3.1) (3.1.0)\n",
    "Installing collected packages: torch-geometric\n",
    "Successfully installed torch-geometric-2.3.1\n",
    "Processing /kaggle/input/fast-slow-4-dataset-train/torch_scatter-2.1.1-cp310-cp310-linux_x86_64.whl\n",
    "Installing collected packages: torch-scatter\n",
    "Successfully installed torch-scatter-2.1.1\n",
    "!pip install timm\n",
    "Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (0.9.7)\n",
    "Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.10/site-packages (from timm) (2.0.0)\n",
    "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.15.1)\n",
    "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm) (6.0)\n",
    "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.4)\n",
    "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.3.3)\n",
    "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.12.2)\n",
    "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (4.6.3)\n",
    "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (1.12)\n",
    "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.1)\n",
    "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.1.2)\n",
    "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (2023.9.0)\n",
    "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (2.31.0)\n",
    "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (4.66.1)\n",
    "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (21.3)\n",
    "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (1.23.5)\n",
    "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (9.5.0)\n",
    "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\n",
    "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
    "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (3.1.0)\n",
    "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (3.4)\n",
    "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (1.26.15)\n",
    "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
    "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
    "\"\"\"\n",
    "import timm\n",
    "from timm.scheduler import  CosineLRScheduler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "\n",
    "import sklearn,sklearn.model_selection\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn import GCNConv,SAGEConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "#from timm.scheduler import CosineLRScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cpu'\n",
    "def load_df(directory):\n",
    "    splits = [\"test\"]\n",
    "    dfs = dict()\n",
    "    \n",
    "    for split in splits:\n",
    "        path = os.path.join(directory, split)\n",
    "        files = os.listdir(path)\n",
    "        list_df = []\n",
    "        \n",
    "        for file in files:\n",
    "            d = dict(np.load(os.path.join(path,file)))\n",
    "            d['file'] = file\n",
    "            list_df.append(d)\n",
    "        dfs[split] = pd.DataFrame.from_dict(list_df)\n",
    "    return dfs\n",
    "layout_xla_random = load_df(\"/kaggle/input/predict-ai-model-runtime/npz_all/npz/layout/xla/random/\")\n",
    "layout_xla_default = load_df(\"/kaggle/input/predict-ai-model-runtime/npz_all/npz/layout/xla/default/\")\n",
    "layout_nlp_default = load_df(\"/kaggle/input/predict-ai-model-runtime/npz_all/npz/layout/nlp/default/\")\n",
    "layout_nlp_random = load_df(\"/kaggle/input/predict-ai-model-runtime/npz_all/npz/layout/nlp/random/\")\n",
    "class TileDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        config_feat = torch.tensor(row['node_config_feat'].astype(np.float32))\n",
    "        node_feat = torch.tensor(row['node_feat'].astype(np.float32))\n",
    "        node_opcode = torch.tensor(row['node_opcode'].astype(np.int64))\n",
    "        edge_index = torch.tensor(np.swapaxes(row['edge_index'],0,1).astype(np.int64))\n",
    "        target = (row['config_runtime']).astype(np.float32)\n",
    "        # minmax scale the target, we only care about order\n",
    "        target = (target-min(target))/(max(target) -min(target))\n",
    "        target = torch.tensor(target)\n",
    "        return config_feat,node_feat,node_opcode,edge_index,target\n",
    "    \n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, graph_feats, hidden_dim):\n",
    "        super().__init__()\n",
    "        op_embedding_dim = 4 # I choose 4-dimensional embedding\n",
    "        self.embedding = torch.nn.Embedding(120, #120 different op-codes\n",
    "                                            op_embedding_dim,\n",
    "                                           )\n",
    "        assert len(hidden_channels)>0\n",
    "        in_channels = op_embedding_dim+140\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        last_dim = hidden_channels[0]\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels[0]))\n",
    "        for i in range(len(hidden_channels)-1):\n",
    "            self.convs.append(GCNConv(hidden_channels[i], hidden_channels[i+1]))\n",
    "            last_dim = hidden_channels[i+1]\n",
    "        self.convs.append(GCNConv(last_dim, graph_feats))\n",
    "        \n",
    "        self.dense = torch.nn.Sequential(nn.Linear(82, 64),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Linear(64, 64),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Linear(64, 1),\n",
    "                                        )\n",
    "    \n",
    "    def forward(self, x_cfg: Tensor,x_feat: Tensor, x_op: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        \n",
    "        #get graph features\n",
    "        x_cfg = x_cfg.mean(dim=1)\n",
    "        #print(x_cfg.shape)\n",
    "        x = torch.concat([x_feat,self.embedding(x_op)],dim = 1)\n",
    "        #pass though conv layers\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index).relu()\n",
    "        # get 1d graph embedding using average pooling\n",
    "        x_graph = torch.mean(x,0)\n",
    "        \n",
    "        \n",
    "        #put graph data into config data\n",
    "        x = torch.concat([x_cfg,x_graph.repeat((len(x_cfg),1))],axis=1) #torch.Size([10528, 225])\n",
    "        #put into dense nn\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(self.dense(x))\n",
    "        return x\n",
    "\n",
    "model = SimpleModel(hidden_channels = [16,32,16,48],graph_feats = 64,hidden_dim=64).to(device)\n",
    "class SimpleModel2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, graph_feats, hidden_dim):\n",
    "        super().__init__()\n",
    "        op_embedding_dim = 4 # I choose 4-dimensional embedding\n",
    "        self.embedding = torch.nn.Embedding(120, #120 different op-codes\n",
    "                                            op_embedding_dim,\n",
    "                                           )\n",
    "        assert len(hidden_channels)>0\n",
    "        in_channels = op_embedding_dim+140\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        last_dim = hidden_channels[0]\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels[0]))\n",
    "        for i in range(len(hidden_channels)-1):\n",
    "            self.convs.append(SAGEConv(hidden_channels[i], hidden_channels[i+1]))\n",
    "            last_dim = hidden_channels[i+1]\n",
    "        self.convs.append(SAGEConv(last_dim, graph_feats))\n",
    "        \n",
    "        self.dense = torch.nn.Sequential(nn.Linear(82, 64),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Linear(64, 64),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Linear(64, 1),\n",
    "                                        )\n",
    "    \n",
    "    def forward(self, x_cfg: Tensor,x_feat: Tensor, x_op: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        \n",
    "        #get graph features\n",
    "        x_cfg = x_cfg.mean(dim=1)\n",
    "        #print(x_cfg.shape)\n",
    "        x = torch.concat([x_feat,self.embedding(x_op)],dim = 1)\n",
    "        #pass though conv layers\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index).relu()\n",
    "        # get 1d graph embedding using average pooling\n",
    "        x_graph = torch.mean(x,0)\n",
    "        \n",
    "        \n",
    "        #put graph data into config data\n",
    "        x = torch.concat([x_cfg,x_graph.repeat((len(x_cfg),1))],axis=1) #torch.Size([10528, 225])\n",
    "        #put into dense nn\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(self.dense(x))\n",
    "        return x\n",
    "\n",
    "model2 = SimpleModel2(hidden_channels = [16,32,16,48],graph_feats = 64,hidden_dim=64).to(device)\n",
    "dataset = TileDataset(layout_xla_default[\"test\"])\n",
    "tile_xla_predictions = [[] for i in range(len(dataset))]\n",
    "for fold in range(5):\n",
    "    model.load_state_dict(torch.load(f'/kaggle/input/fast-slow-sep/xla_defalut/layout_xla_default_best_model_{fold}.pth',map_location=torch.device('cpu') ))\n",
    "    model.eval()\n",
    "    pbar = tqdm(range(len(dataset)))\n",
    "    for i in pbar:\n",
    "        cfg_ft,nd_ft,nd_op,ind,target = dataset[i]\n",
    "        cfg_ft,nd_ft,nd_op,ind,target = cfg_ft.to(device),nd_ft.to(device),nd_op.to(device),ind.to(device),target.to(device)\n",
    "\n",
    "        out = model(cfg_ft,nd_ft,nd_op,ind)\n",
    "        tile_xla_predictions[i].append(out.cpu().detach().numpy())\n",
    "tile_xla_predictions = [np.argsort(np.mean(pred,axis=0))[:-1] for pred in tile_xla_predictions]\n",
    "tile_xla_predictions[0]\n",
    "#sub = submission_df\n",
    "sub = pd.read_csv('/kaggle/input/predict-ai-model-runtime/sample_submission.csv')\n",
    "for i,filename in enumerate(layout_xla_random[\"test\"]['file'].values):\n",
    "    id = 'layout:xla:default:' +filename[:-4]\n",
    "    print(id)\n",
    "    sub.loc[sub.ID == id,'TopConfigs'] = ';'.join(tile_xla_predictions[i].astype(str))\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "sub\n",
    "\"\"\"\n",
    "  0%|          | 0/8 [00:00<?, ?it/s]/tmp/ipykernel_23/1750813363.py:16: RuntimeWarning: invalid value encountered in divide\n",
    "  target = (target-min(target))/(max(target) -min(target))\n",
    "100%|██████████| 8/8 [00:01<00:00,  5.14it/s]\n",
    "100%|██████████| 8/8 [00:01<00:00,  5.07it/s]\n",
    "100%|██████████| 8/8 [00:01<00:00,  5.20it/s]\n",
    "100%|██████████| 8/8 [00:01<00:00,  5.45it/s]\n",
    "100%|██████████| 8/8 [00:01<00:00,  5.45it/s]\n",
    "layout:xla:default:cd708819d3f5103afd6460b15e74eaf3\n",
    "layout:xla:default:05ae41e26dd3c4c06390371a0423233c\n",
    "layout:xla:default:e8a3a1401b5e79f66d7037e424f3b6df\n",
    "layout:xla:default:fbaa8bb6a1aed9988281085c91065c05\n",
    "layout:xla:default:937ee0eb0d5d6151b7b8252933b5c1c9\n",
    "layout:xla:default:3e7156ac468dfb75cf5c9615e1e5887d\n",
    "layout:xla:default:5335ed13823b0a518ee3c79ba4425f34\n",
    "layout:xla:default:db59a991b7c607634f13570d52ce885f\n",
    "ID\tTopConfigs\n",
    "0\ttile:xla:d6f5f54247bd1e58a10b9e7062c636ab\t0;1;2;3;4\n",
    "1\ttile:xla:e3a655daa38e34ec240df959b650ac16\t0;1;2;3;4\n",
    "2\ttile:xla:f8c2c1a1098b2a361c26df668b286c87\t0;1;2;3;4\n",
    "3\ttile:xla:4dd1716853ed46ee4e7d09ede1732de8\t0;1;2;3;4\n",
    "4\ttile:xla:d0a69155b6340748c36724e4bfc34be3\t0;1;2;3;4\n",
    "...\t...\t...\n",
    "889\tlayout:nlp:random:60880ed76de53f4d7a1b960b24f2...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "890\tlayout:nlp:random:23559853d9702baaaacbb0c83fd3...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "891\tlayout:nlp:random:f6c146fc5cf10be4f3accbaca989...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "892\tlayout:nlp:random:32531d07a084b319dce484f53a4c...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "893\tlayout:nlp:random:3a0c5517a87df8d82fd637b83298...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "894 rows × 2 columns\n",
    "\"\"\"\n",
    "dataset = TileDataset(layout_xla_random[\"test\"])\n",
    "tile_xla_predictions = [[] for i in range(len(dataset))]\n",
    "for fold in range(5):\n",
    "    model.load_state_dict(torch.load(f'/kaggle/input/fast-slow-sep/xla_random/layout_xla_default_best_model_{fold}.pth',map_location=torch.device('cpu') ))\n",
    "    model.eval()\n",
    "    pbar = tqdm(range(len(dataset)))\n",
    "    for i in pbar:\n",
    "        cfg_ft,nd_ft,nd_op,ind,target = dataset[i]\n",
    "        cfg_ft,nd_ft,nd_op,ind,target = cfg_ft.to(device),nd_ft.to(device),nd_op.to(device),ind.to(device),target.to(device)\n",
    "\n",
    "        out = model(cfg_ft,nd_ft,nd_op,ind)\n",
    "        tile_xla_predictions[i].append(out.cpu().detach().numpy())\n",
    "tile_xla_predictions = [np.argsort(np.mean(pred,axis=0))[:-1] for pred in tile_xla_predictions]\n",
    "tile_xla_predictions[0]\n",
    "\n",
    "#sub = pd.read_csv('/kaggle/input/predict-ai-model-runtime/sample_submission.csv')\n",
    "for i,filename in enumerate(layout_xla_random[\"test\"]['file'].values):\n",
    "    id = 'layout:xla:random:' +filename[:-4]\n",
    "    print(id)\n",
    "    sub.loc[sub.ID == id,'TopConfigs'] = ';'.join(tile_xla_predictions[i].astype(str))\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "sub\n",
    "\"\"\"\n",
    "  0%|          | 0/8 [00:00<?, ?it/s]/tmp/ipykernel_23/1750813363.py:16: RuntimeWarning: invalid value encountered in divide\n",
    "  target = (target-min(target))/(max(target) -min(target))\n",
    "100%|██████████| 8/8 [00:01<00:00,  5.35it/s]\n",
    "100%|██████████| 8/8 [00:01<00:00,  5.63it/s]\n",
    "100%|██████████| 8/8 [00:02<00:00,  3.33it/s]\n",
    "100%|██████████| 8/8 [00:01<00:00,  5.74it/s]\n",
    "100%|██████████| 8/8 [00:01<00:00,  5.69it/s]\n",
    "layout:xla:random:cd708819d3f5103afd6460b15e74eaf3\n",
    "layout:xla:random:05ae41e26dd3c4c06390371a0423233c\n",
    "layout:xla:random:e8a3a1401b5e79f66d7037e424f3b6df\n",
    "layout:xla:random:fbaa8bb6a1aed9988281085c91065c05\n",
    "layout:xla:random:937ee0eb0d5d6151b7b8252933b5c1c9\n",
    "layout:xla:random:3e7156ac468dfb75cf5c9615e1e5887d\n",
    "layout:xla:random:5335ed13823b0a518ee3c79ba4425f34\n",
    "layout:xla:random:db59a991b7c607634f13570d52ce885f\n",
    "ID\tTopConfigs\n",
    "0\ttile:xla:d6f5f54247bd1e58a10b9e7062c636ab\t0;1;2;3;4\n",
    "1\ttile:xla:e3a655daa38e34ec240df959b650ac16\t0;1;2;3;4\n",
    "2\ttile:xla:f8c2c1a1098b2a361c26df668b286c87\t0;1;2;3;4\n",
    "3\ttile:xla:4dd1716853ed46ee4e7d09ede1732de8\t0;1;2;3;4\n",
    "4\ttile:xla:d0a69155b6340748c36724e4bfc34be3\t0;1;2;3;4\n",
    "...\t...\t...\n",
    "889\tlayout:nlp:random:60880ed76de53f4d7a1b960b24f2...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "890\tlayout:nlp:random:23559853d9702baaaacbb0c83fd3...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "891\tlayout:nlp:random:f6c146fc5cf10be4f3accbaca989...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "892\tlayout:nlp:random:32531d07a084b319dce484f53a4c...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "893\tlayout:nlp:random:3a0c5517a87df8d82fd637b83298...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "894 rows × 2 columns\n",
    "\"\"\"\n",
    "dataset = TileDataset(layout_nlp_default[\"test\"])\n",
    "tile_xla_predictions = [[] for i in range(len(dataset))]\n",
    "for fold in range(5):\n",
    "    model.load_state_dict(torch.load(f'/kaggle/input/fast-slow-nlp-v3/nlp_default/layout_xla_default_best_model_{fold}.pth',map_location=torch.device('cpu') ))\n",
    "    model.eval()\n",
    "    pbar = tqdm(range(len(dataset)))\n",
    "    for i in pbar:\n",
    "        cfg_ft,nd_ft,nd_op,ind,target = dataset[i]\n",
    "        cfg_ft,nd_ft,nd_op,ind,target = cfg_ft.to(device),nd_ft.to(device),nd_op.to(device),ind.to(device),target.to(device)\n",
    "        out = model(cfg_ft,nd_ft,nd_op,ind) \n",
    "        tile_xla_predictions[i].append(out.cpu().detach().numpy())\n",
    "tile_xla_predictions = [np.argsort(np.mean(pred,axis=0))[:-1] for pred in tile_xla_predictions]\n",
    "tile_xla_predictions[0]\n",
    "\n",
    "#sub = pd.read_csv('/kaggle/input/predict-ai-model-runtime/sample_submission.csv')\n",
    "for i,filename in enumerate(layout_nlp_default[\"test\"]['file'].values):\n",
    "    id = 'layout:nlp:default:' +filename[:-4]\n",
    "    print(id)\n",
    "    sub.loc[sub.ID == id,'TopConfigs'] = ';'.join(tile_xla_predictions[i].astype(str))\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "sub\n",
    "\"\"\"\n",
    "  0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_23/1750813363.py:16: RuntimeWarning: invalid value encountered in divide\n",
    "  target = (target-min(target))/(max(target) -min(target))\n",
    "100%|██████████| 17/17 [00:00<00:00, 42.06it/s]\n",
    "100%|██████████| 17/17 [00:00<00:00, 40.82it/s]\n",
    "100%|██████████| 17/17 [00:00<00:00, 33.71it/s]\n",
    "100%|██████████| 17/17 [00:00<00:00, 36.62it/s]\n",
    "100%|██████████| 17/17 [00:00<00:00, 42.31it/s]\n",
    "layout:nlp:default:b2fdde3b72980907578648774101543e\n",
    "layout:nlp:default:29886a50d55cfe77a9497bc906c76ce9\n",
    "layout:nlp:default:7105451001e119f65b66570d170b94a8\n",
    "layout:nlp:default:171b0513d8874a427ccfa46d136fbadc\n",
    "layout:nlp:default:60880ed76de53f4d7a1b960b24f20f7d\n",
    "layout:nlp:default:58cc2e418c3a8a19b871e15964b534ad\n",
    "layout:nlp:default:f6c146fc5cf10be4f3accbaca9897311\n",
    "layout:nlp:default:38524e2ff135ded55b5286407e7af6b7\n",
    "layout:nlp:default:3a0c5517a87df8d82fd637b83298a3ba\n",
    "layout:nlp:default:6c1101f6231f4d1722c3b9f6d1e25026\n",
    "layout:nlp:default:016ac66a44a906a695afd2228509046a\n",
    "layout:nlp:default:492c7a94d559aa4a88769142d2a68362\n",
    "layout:nlp:default:d15316c12eefdef1ba549eb433797f77\n",
    "layout:nlp:default:7f6284ebe027b1e9a3850fc703858a59\n",
    "layout:nlp:default:32531d07a084b319dce484f53a4cf3fc\n",
    "layout:nlp:default:23559853d9702baaaacbb0c83fd32266\n",
    "layout:nlp:default:71b79ca6db513e7979c3702c595150c2\n",
    "ID\tTopConfigs\n",
    "0\ttile:xla:d6f5f54247bd1e58a10b9e7062c636ab\t0;1;2;3;4\n",
    "1\ttile:xla:e3a655daa38e34ec240df959b650ac16\t0;1;2;3;4\n",
    "2\ttile:xla:f8c2c1a1098b2a361c26df668b286c87\t0;1;2;3;4\n",
    "3\ttile:xla:4dd1716853ed46ee4e7d09ede1732de8\t0;1;2;3;4\n",
    "4\ttile:xla:d0a69155b6340748c36724e4bfc34be3\t0;1;2;3;4\n",
    "...\t...\t...\n",
    "889\tlayout:nlp:random:60880ed76de53f4d7a1b960b24f2...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "890\tlayout:nlp:random:23559853d9702baaaacbb0c83fd3...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "891\tlayout:nlp:random:f6c146fc5cf10be4f3accbaca989...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "892\tlayout:nlp:random:32531d07a084b319dce484f53a4c...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "893\tlayout:nlp:random:3a0c5517a87df8d82fd637b83298...\t0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18...\n",
    "894 rows × 2 columns\n",
    "\n",
    "43のコードをオリジナルでエラーが出たので改変した。\n",
    "\"\"\"\n",
    "dataset = TileDataset(layout_nlp_random[\"test\"])\n",
    "tile_xla_predictions = [[] for i in range(len(dataset))]\n",
    "for fold in range(2):\n",
    "    model.load_state_dict(torch.load(f'/kaggle/input/fast-slow-sep/nlp_random/layout_xla_default_best_model_{fold}.pth',map_location=torch.device('cpu') ))\n",
    "    model.eval()\n",
    "    \n",
    "    pbar = tqdm(range(len(dataset)))\n",
    "    for i in pbar:\n",
    "        cfg_ft,nd_ft,nd_op,ind,target = dataset[i]\n",
    "        cfg_ft,nd_ft,nd_op,ind,target = cfg_ft.to(device),nd_ft.to(device),nd_op.to(device),ind.to(device),target.to(device)\n",
    "\n",
    "        out = model(cfg_ft,nd_ft,nd_op,ind) \n",
    "        tile_xla_predictions[i].append(out.cpu().detach().numpy())\n",
    "tile_xla_predictions = [np.argsort(np.mean(pred,axis=0))[:-1] for pred in tile_xla_predictions]\n",
    "tile_xla_predictions[0]\n",
    "\n",
    "#sub = pd.read_csv('/kaggle/input/predict-ai-model-runtime/sample_submission.csv')\n",
    "for i,filename in enumerate(layout_nlp_random[\"test\"]['file'].values):\n",
    "    id = 'layout:nlp:random:' +filename[:-4]\n",
    "    print(id)\n",
    "    sub.loc[sub.ID == id,'TopConfigs'] = ';'.join(tile_xla_predictions[i].astype(str))\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "sub\n",
    "\"\"\"\n",
    "  0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_23/1750813363.py:16: RuntimeWarning: invalid value encountered in divide\n",
    "  target = (target-min(target))/(max(target) -min(target))\n",
    "100%|██████████| 17/17 [00:00<00:00, 38.68it/s]\n",
    "100%|██████████| 17/17 [00:00<00:00, 39.14it/s]\n",
    "layout:nlp:random:b2fdde3b72980907578648774101543e\n",
    "layout:nlp:random:29886a50d55cfe77a9497bc906c76ce9\n",
    "layout:nlp:random:7105451001e119f65b66570d170b94a8\n",
    "layout:nlp:random:171b0513d8874a427ccfa46d136fbadc\n",
    "layout:nlp:random:60880ed76de53f4d7a1b960b24f20f7d\n",
    "layout:nlp:random:58cc2e418c3a8a19b871e15964b534ad\n",
    "layout:nlp:random:f6c146fc5cf10be4f3accbaca9897311\n",
    "layout:nlp:random:38524e2ff135ded55b5286407e7af6b7\n",
    "layout:nlp:random:3a0c5517a87df8d82fd637b83298a3ba\n",
    "layout:nlp:random:6c1101f6231f4d1722c3b9f6d1e25026\n",
    "layout:nlp:random:016ac66a44a906a695afd2228509046a\n",
    "layout:nlp:random:492c7a94d559aa4a88769142d2a68362\n",
    "layout:nlp:random:d15316c12eefdef1ba549eb433797f77\n",
    "layout:nlp:random:7f6284ebe027b1e9a3850fc703858a59\n",
    "layout:nlp:random:32531d07a084b319dce484f53a4cf3fc\n",
    "layout:nlp:random:23559853d9702baaaacbb0c83fd32266\n",
    "layout:nlp:random:71b79ca6db513e7979c3702c595150c2\n",
    "ID\tTopConfigs\n",
    "0\ttile:xla:d6f5f54247bd1e58a10b9e7062c636ab\t0;1;2;3;4\n",
    "1\ttile:xla:e3a655daa38e34ec240df959b650ac16\t0;1;2;3;4\n",
    "2\ttile:xla:f8c2c1a1098b2a361c26df668b286c87\t0;1;2;3;4\n",
    "3\ttile:xla:4dd1716853ed46ee4e7d09ede1732de8\t0;1;2;3;4\n",
    "4\ttile:xla:d0a69155b6340748c36724e4bfc34be3\t0;1;2;3;4\n",
    "...\t...\t...\n",
    "889\tlayout:nlp:random:60880ed76de53f4d7a1b960b24f2...\t17;604;850;9;150;326;639;288;23;286;627;923;57...\n",
    "890\tlayout:nlp:random:23559853d9702baaaacbb0c83fd3...\t62;976;644;52;218;38;30;275;916;318;618;74;893...\n",
    "891\tlayout:nlp:random:f6c146fc5cf10be4f3accbaca989...\t208;615;715;367;967;594;408;363;649;282;767;55...\n",
    "892\tlayout:nlp:random:32531d07a084b319dce484f53a4c...\t212;76;849;778;847;4;396;482;787;605;934;273;7...\n",
    "893\tlayout:nlp:random:3a0c5517a87df8d82fd637b83298...\t869;518;130;854;967;998;876;240;582;187;573;25...\n",
    "894 rows × 2 columns\n",
    "\n",
    "\"\"\"\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
